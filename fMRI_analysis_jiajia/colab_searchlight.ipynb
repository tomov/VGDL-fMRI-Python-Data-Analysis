{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "07_searchlight.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Eq0UDGr3wcS2",
        "j4vdA5uvwcS6"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomov/VGDL-fMRI-Python-Data-Analysis/blob/master/fMRI_analysis_jiajia/colab_searchlight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "4988daf4-346a-4a15-b8f8-4802ccdfb87b"
        },
        "id": "hHDB9zvmwcRd",
        "colab_type": "text"
      },
      "source": [
        "# Searchlights <a name=\"searchlights\"></a>\n",
        "[Contributions](#contributions)\n",
        "\n",
        "In univariate analyses there is a distinction between whole-brain and [ROI-based](https://doi.org/10.1093/scan/nsm006) analyses. When done correctly, whole-brain univariate analyses allow you to identify, in an unbiased and data-driven way, regions of the brain that differ between experimental conditions. Compare this to what we have done with multivariate classification: up to this point we have taken ROIs (e.g., FFA, PPA, even the whole brain) and looked at the pattern of activity across voxels. Using these procedures we have been able to determine whether these voxels collectively contain information about different conditions. However, throughout these analyses it has been unclear whether a subset of these voxels have been driving classification accuracy. Although we could use the weights to indicate which voxels are important, weights alone do not always convey the influence of individual voxels. In other words, we have been unable to say with certainty *where* in the brain there are representations that distinguish between conditions. \n",
        "\n",
        "A searchlight is a spatial moving window that exhaustively searches the brain in order to localize representations. Running a searchlight is computationally intensive because it involves running a separate multivariate analysis for every voxel in the brain. Imagine doing your feature selection, hyper-parameter optimization, and cross-validation 100,000 times or more. Fortunately, [BrainIAK](http://brainiak.org/) contains a procedure that efficiently carves up brain data into appropriate chunks and distributes them across the computational resources available.\n",
        "\n",
        "For more information on the searchlight technique, read this comprehensive [NeuroImage Comment](https://doi.org/10.1016/j.neuroimage.2013.03.041), that also includes citations to landmark papers on the topic. The validity of using searchlights is explained [here](https://doi.org/10.1073/pnas.0600244103).\n",
        " \n",
        "**Pre-requisites:** Familiarity with data loading and classification, notebooks 1-5.\n",
        "\n",
        "## Goal of this script\n",
        "\n",
        "1. Learn how to perform a whole-brain searchlight.  \n",
        "2. Learn how to replace the kernel used inside the searchlight.  \n",
        "3. Use batch scripts, SLURM, and MPI to run searchlight on compute clusters.\n",
        "4. Run searchlight on a face-scene dataset.\n",
        "\n",
        "## Table of contents\n",
        "\n",
        ">1. [Searchlight workflow](#sl_wf)  \n",
        ">>1.1 [Data preparation](#data_prep)  \n",
        ">>1.2 [Executing the searchlight workflow](#exe_wf)   \n",
        ">>1.3 [Executing on multiple subjects](#multi_subj)\n",
        ">2. [Running searchlight analyses on a cluster](#submitting_searchlights)  \n",
        ">>2.1 [Distributing jobs with parallel computing](#ranks)\n",
        "\n",
        "> 4.Exercises\n",
        ">[1](#ex1)   [2](#ex2)  [3](#ex3)  [4](#ex4)  [5](#ex5)  [6](#ex6)  [7](#ex7)  [8](#ex8)  [9](#ex9) [10](#ex10) [11](#ex11) [12](#ex12)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9Yqp1AbxrfD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fbe1ea85-cae1-425a-85a9-457e6ca1d412"
      },
      "source": [
        "!pip install deepdish ipython matplotlib nilearn notebook pandas seaborn watchdog\n",
        "!pip install pip\\<10\n",
        "!pip install -U git+https://github.com/brainiak/brainiak\n",
        "!git clone https://github.com/brainiak/brainiak-tutorials.git\n",
        "!cd brainiak-tutorials/tutorials/; cp -r 07-searchlight 09-fcma 13-real-time utils.py setup_environment.sh /content/\n",
        "!mkdir /root/brainiak_datasets"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deepdish in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: nilearn in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deepdish)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.6/dist-packages (from deepdish)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from deepdish)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from nilearn)\n",
            "Requirement already satisfied: nibabel>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from nilearn)\n",
            "Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.6/dist-packages (from nilearn)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from nilearn)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from notebook)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from notebook)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from notebook)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas)\n",
            "Requirement already satisfied: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from tables->deepdish)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.6/dist-packages (from tables->deepdish)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.6/dist-packages (from nibabel>=2.0.2->nilearn)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19->nilearn)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.0->notebook)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook)\n",
            "Requirement already satisfied: pip<10 in /usr/local/lib/python3.6/dist-packages\n",
            "Collecting git+https://github.com/brainiak/brainiak\n",
            "  Cloning https://github.com/brainiak/brainiak to /tmp/pip-kp_rdv_2-build\n",
            "/usr/local/lib/python3.6/dist-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
            "  \"Distutils was imported before Setuptools. This usage is discouraged \"\n",
            "Requirement already up-to-date: cython in /usr/local/lib/python3.6/dist-packages (from brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: mpi4py>=3 in /usr/local/lib/python3.6/dist-packages (from brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: nitime in /usr/local/lib/python3.6/dist-packages (from brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: numpy!=1.17.* in /usr/local/lib/python3.6/dist-packages (from brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: scikit-learn[alldeps]>=0.18 in /usr/local/lib/python3.6/dist-packages (from brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: scipy!=1.0.0 in /usr/local/lib/python3.6/dist-packages (from brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: statsmodels in /usr/local/lib/python3.6/dist-packages (from brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: pymanopt in /usr/local/lib/python3.6/dist-packages (from brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: theano>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: pybind11>=1.7 in /usr/local/lib/python3.6/dist-packages (from brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: psutil in /usr/local/lib/python3.6/dist-packages (from brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: nibabel in /usr/local/lib/python3.6/dist-packages (from brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: joblib in /usr/local/lib/python3.6/dist-packages (from brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: wheel in /usr/local/lib/python3.6/dist-packages (from brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: pydicom in /usr/local/lib/python3.6/dist-packages (from brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: networkx in /usr/local/lib/python3.6/dist-packages (from nitime->brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: matplotlib in /usr/local/lib/python3.6/dist-packages (from nitime->brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn[alldeps]>=0.18->brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: pandas>=0.21 in /usr/local/lib/python3.6/dist-packages (from statsmodels->brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: patsy>=0.5 in /usr/local/lib/python3.6/dist-packages (from statsmodels->brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from theano>=1.0.4->brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: packaging>=14.3 in /usr/local/lib/python3.6/dist-packages (from nibabel->brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->nitime->brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nitime->brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nitime->brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nitime->brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nitime->brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nitime->brainiak==0.11.dev9+g7614892)\n",
            "Requirement already up-to-date: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21->statsmodels->brainiak==0.11.dev9+g7614892)\n",
            "Installing collected packages: brainiak\n",
            "  Found existing installation: brainiak 0.11.dev9+g7614892\n",
            "    Uninstalling brainiak-0.11.dev9+g7614892:\n",
            "      Successfully uninstalled brainiak-0.11.dev9+g7614892\n",
            "  Running setup.py install for brainiak ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed brainiak-0.11.dev9+g7614892\n",
            "fatal: destination path 'brainiak-tutorials' already exists and is not an empty directory.\n",
            "mkdir: cannot create directory ‘/root/brainiak_datasets’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-fs6vezwcRe",
        "colab_type": "text"
      },
      "source": [
        "### Dataset <a id=\"data-set\"></a> \n",
        "\n",
        "For this script we will use the face/scene dataset from [Wang et al. (2015)](https://doi.org/10.1016/j.jneumeth.2015.05.012), who in turn used localizer data from [Turk-Browne et al. (2012)](https://doi.org/10.1523/JNEUROSCI.0942-12.2012).\n",
        "\n",
        "Localizer details from Turk-Browne et al. (2012):\n",
        "\n",
        "\"Subjects also completed a functional localizer run lasting 6 min 6 s [...] The localizer run alternated between six repetitions each of scene and face blocks (Turk-Browne et al., 2010). Blocks contained 12 stimuli presented for 500 ms every 1500 ms. The 18 s of stimulation was followed by 12 s of fixation before the next block. Subjects made the same indoor/outdoor judgment for scenes, and a male/female judgment for faces.\"\n",
        "\n",
        "**Self-study:** Explore the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oPCCOaAwcRf",
        "colab_type": "text"
      },
      "source": [
        "## 1. Searchlight workflow <a id=\"sl_wf\"></a>\n",
        "\n",
        "Running a searchlight is computationally intensive, complex and involves many steps. To show how the searchlight functionality in BrainIAK works, we will walk through the steps needed to perform a simple searchlight that can be run in this notebook before moving onto a real example that requires submitting the job to a cluster. This simple workflow takes the following steps:\n",
        "\n",
        ">1. [Data preparation](#data_prep)\n",
        ">2. [Set the searchlight parameters](#set_param)\n",
        ">3. [Create the searchlight object](#create_obj)\n",
        ">4. [Create our classification kernel](#create_kernel)\n",
        ">5. [Execute the searchlight](#exec_sl)\n",
        "\n",
        "\n",
        "However, before we start, there are a few things that you should know about searchlights. Think of a searchlight as a processing step in which you pull out a certain size chunk of your data and then perform a kernel operation (specified by a function you write). You can use any kernel on this chunk of data interchangeably. Critically, **the searchlight function does not specify the analysis you want to perform, all it does is carve up the data.**\n",
        "\n",
        "**Computational demand and parallelization**\n",
        "\n",
        "As mentioned before, searchlights are computationally intensive and so we need to be aware of what kind of burden they might impose. With each subject, we want to perform the operation thousands of times (once for each of their brain voxels). If we were to run this serially, even if each operation only took 1 s to complete, the analysis would take multiple days for only one participant. With nested cross-validation or other recursive steps, the full analysis could take months or years (and you'd lose a lot of points on your assignment).\n",
        "\n",
        "Fortunately, the searchlight function in [BrainIAK](http://brainiak.org/) intelligently parallelizes your data to give you a considerable and scalable speed boost. Parallelization is the idea that when two or more computational tasks can be completed independently (because they don't interact in any way) then it is possible to run these tasks simultaneously on different cores. Note that we refer to cores as our unit of serial processing, although there are other parallelizations available within-core, such as threads or even multiple instructions within thread. The nice thing about parallelization is that it is scalable: in general, a job parallelized across 10 cores will run up to 10 times faster than on one core. For reference, your computer likely has 2, 4, or more cores, so you could speed up processing if you recruited all of these resources (and shut down all other types of background processing). \n",
        "<div class=\"alert alert-block alert-info\">\n",
        "So remember, the two main things that determine the speed of a searchlight: the **kernel algorithm** and the **amount of parallelization**.\n",
        "</div>\n",
        "\n",
        "**How does the BrainIAK searchlight work?**\n",
        "\n",
        "To analyze data efficiently, it is important to understand how the Searchlight function in [BrainIAK](http://brainiak.org/) works. You must provide the function with a numpy volume of 4D data and a binary mask of brain voxels. The searchlight code simply searches for every voxel in the mask that is equal to 1 and then centers a searchlight around this. This means that for every value of 1 in your mask, the searchlight function will apply your kernel function to the corresponding voxel (and those around it) in the 4D volume. Hence when writing the kernel you need to keep in mind that the input data the function receives are not the size of the original data but instead the size of the searchlight radius. In other words, you might give the searchlight function a brain and mask volume that is 64x64x64 but each kernel operation only runs on a data and mask volume that is 3x3x3 (or whatever your searchlight radius is). As per the logic of searchlights, the center of each mask in the searchlight will be equal to 1 (otherwise the searchlight wouldn't have selected it). \n",
        "\n",
        "**How to start using a searchlight? Small!**\n",
        "\n",
        "When getting used to searchlights it is encouraged that you scale up your code gently. This is to prevent the possibility that you run a searchlight that takes a very long time to finish only to find there was a small error (which happens a lot). Instead it is better to write a simple function that runs quickly so you can check if your code works properly before scaling up. A simple workflow for testing your searchlight (and estimating how long your searchlight would take on a bigger dataset) would be this:\n",
        "\n",
        "1. Create a mask of one voxel and run the searchlight interactively (like we are doing now, using a notebook) to check whether the code works.\n",
        "2. Use timestamps to extract the execution time  \n",
        "3. Print the number of voxels that are passed to the searchlight function  \n",
        "4. Run the searchlight as a job on the smallest unit of real data you have (a single run or single participant)\n",
        "5. Check the runtime and memory usage of this searchlight (e.g. on slurm: `sacct -j $JID --format=jobid,maxvmsize,elapsed`).  \n",
        "\n",
        "Taking our own advice, we are going to write a searchlight script below to perform a very simple searchlight. In fact we are only going to run this searchlight on one voxel in the brain so that we can see whether our code is working and how long each kernel operation would take. After we have familiarized ourselves with the way searchlights work on a small scale, we will then graduate to full-scale analyses using batch scripts and cluster computing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DOCk4YXwcRg",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Data preparation <a id=\"data_prep\"></a>\n",
        "Prepare a single participant using a similar pipeline to what we have developed previously. The critical change needed here is the shape of the data: In the past we wanted a time x voxel matrix, but the input to a searchlight is a 4D volume (brain x, y, z, time t), hence we need to add a step, as below.\n",
        "\n",
        "<a id=\"ex1\"></a>\n",
        "**Exercise 1:** Why does the input to a searchlight analysis need to be 4D rather than 2D? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azM4YG72wcRg",
        "colab_type": "text"
      },
      "source": [
        "**A:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1wV3-aHwcRh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "973d99d2-d558-478f-f18e-bbf374955e40"
      },
      "source": [
        "import h5py\n",
        "import warnings\n",
        "import sys \n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    \n",
        "# Import libraries\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import os \n",
        "import time\n",
        "from nilearn import plotting\n",
        "from brainiak.searchlight.searchlight import Searchlight\n",
        "from brainiak.fcma.preprocessing import prepare_searchlight_mvpa_data\n",
        "from brainiak import io\n",
        "from brainiak import image\n",
        "from pathlib import Path\n",
        "from shutil import copyfile\n",
        "from nilearn.input_data import NiftiMasker, NiftiLabelsMasker\n",
        "\n",
        "# Import machine learning libraries\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "import scipy.stats\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set printing precision\n",
        "np.set_printoptions(precision=2, suppress=True)\n",
        "\n",
        "%matplotlib inline\n",
        "%matplotlib notebook\n",
        "%autosave 5\n",
        "sns.set(style = 'white', context='poster', rc={\"lines.linewidth\": 2.5})\n",
        "sns.set(palette=\"colorblind\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "IPython.notebook.set_autosave_interval(5000)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Autosaving every 5 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPgeoRk1wcRm",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.1 Helper functions <a id=\"helper\"> </a>\n",
        "\n",
        "To make it easy for you to achieve the main goals of this notebook, we have created helper functions that perform data extraction from nifti files data, format them into arrays, and prepares the data for the searchlight function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYpGDwCGwcR0",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.2 Create a directory structure<a id=\"helper\"> </a>\n",
        "\n",
        "We strongly recommend storing the data and results generated by the analysis in separate directories from the raw data (here we have enforced this by making the raw data directory read-only). On clusters these kinds of intermediate products should be stored on high-speed `scratch` disks. Typically, once your analysis is finished, you would copy the results and any important intermediate steps back to more permanent storage like your home directory or a project directory.\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "Just like a whiteboard is wiped regularly, scratch storage is typically erased after a month or two. Moreover, scratch space is not backed up, so you could commit your work regularly, at a minimum after every session.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_eN6fCpwcR1",
        "colab_type": "text"
      },
      "source": [
        "Please specify the path to a useable `scratch` folder:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOhXuAgJwcSF",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"ex2\"></a>\n",
        "**Exercise 2.** Print out the labels for one subject to get an understanding of the stimulus presentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NplU_qh2wcSJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 1.2 Executing the searchlight workflow<a id=\"exe_wf\"></a>\n",
        "### 1.2.1 Set searchlight parameters <a id=\"set_param\"></a>\n",
        "\n",
        "To run the [searchlight](http://brainiak.org/docs/brainiak.searchlight.html) function in BrainIAK you need the following parameters:  \n",
        "\n",
        "1. **data** = The brain data as a 4D volume.  \n",
        "2. **mask** = A binary mask specifying the \"center\" voxels in the brain around which you want to perform searchlight analyses. A searchlight will be drawn around every voxel with the value of 1. Hence, if you chose to use the wholebrain mask as the mask for the searchlight procedure, the searchlight may include voxels outside of your mask when the \"center\" voxel is at the border of the mask. It is up to you to decide whether then to include these results.  \n",
        "3. **bcvar** = An additional variable which can be a list, numpy array, dictionary, etc. you want to use in your searchlight kernel. For instance you might want the condition labels so that you can determine to which condition each 3D volume corresponds. If you don't need to broadcast anything, e.g, when doing RSA, set this to 'None'.  \n",
        "4. **sl_rad** = The size of the searchlight's radius, excluding the center voxel. This means the total volume size of the searchlight, if using a cube, is defined as: ((2 * sl_rad) + 1) ^ 3.  \n",
        "5. **max_blk_edge** = When the searchlight function carves the data up into chunks, it doesn't distribute only a single searchlight's worth of data. Instead, it creates a block of data, with the edge length specified by this variable, which determines the number of searchlights to run within a job.  \n",
        "6. **pool_size** = Maximum number of cores running on a block (typically 1).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yruoev1RwcSJ",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"ex2\"></a>\n",
        "**Exercise 2:** Searchlights don't need to be cubes. What other shape(s) does BrainIAK support and how do you specify this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwbToYw9wcSK",
        "colab_type": "text"
      },
      "source": [
        "**A:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtAto__PwcSO",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.2 Create Searchlight Object  <a id=\"create_obj\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nrI94EbwcSS",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.3 Define the function (a.k.a. \"kernel\") that needs to be computed <a id=\"create_kernel\"></a>\n",
        "\n",
        "For every searchlight you need a function to run in the searchlight. This is the function that you want to measure/classify your data with. This could perform classification, RSA, or any other computation that you wish.\n",
        "\n",
        "**NB.** The word \"kernel\" is used in multiple mathematical/computational contexts. In this notebook we use kernel to represent the function that is passed to the searchlight object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTA_lNx7wcSW",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.4 Execute the searchlight <a id=\"exec_sl\"></a>\n",
        "\n",
        "We execute the searchlight and save the results in brain space. Each searchlight result is assigned to the voxel that the searchlight is centered on. This array is then saved to a nifti volume using the following:\n",
        "\n",
        "> `nib.Nifti1Image`: Created a nifti volume with the same size and in the same space (e.g. MNI) as the subject data.  \n",
        "> `.header`: This gets header information from the subject data.  \n",
        "> `.set_zooms`: Sets the voxel dimensions e.g 1.5mm x 1.5mm x 1.5mm.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABa3fUzMwcSa",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"ex3\"></a>\n",
        "**Exercise 3:** What would setting `sl_rad` to 3 do? (Please still use `sl_rad = 1` in the following exercises, we just want you to state the answer.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aot4bp8fwcSa",
        "colab_type": "text"
      },
      "source": [
        "**A:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UsGWTmnwcSb",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"ex4\"></a>\n",
        "**Exercise 4:** Estimate how long it would take to run the searchlight analysis above on the whole brain, using the same parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJu2K743wcSc",
        "colab_type": "text"
      },
      "source": [
        "**A:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8orHIu2BwcSd",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"ex5\"></a>\n",
        "**Exercise 5:** Modify the ``small_mask`` variable to select a 4x4x4 sub-volume of 64 voxels (as the searchlight centers) around the voxel in the example above. Hint: You do not need to change the ``sl_rad`` variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZwmhHprwcSg",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"ex6\"></a>\n",
        "**Exercise 6:** Modify the calc_svm function to run only if there are at least 14 \"brain\" voxels from the mask in the 27-voxel searchlight centered on each of these 64 voxels, otherwise return -1. This could be useful if you wanted to ignore searchlights near the edge of the brain. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dURZnuDawcSj",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"ex7\"></a>\n",
        "**Exercise 7:** Modify this new calc_svm function to additionally perform nested cross-validation (use the code you made for the notebook on classification) over different cost parameters of the linear SVM kernel. Note that we do not have ``run_ids`` as in the previous notebooks, so you should use the [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) to perform the cross-validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROXyImFgwcSu",
        "colab_type": "text"
      },
      "source": [
        "**Visualizing the data**\n",
        "\n",
        "As you have seen, the output of the searchlight analysis is a 3D volume with the outputs of your kernel for each voxel it was centered on. The [nilearn package](http://nilearn.github.io/plotting/index.html) has multiple plotting options to allow you to view your data within python. This is good for quick visualizations but can be buggy and is not great for exploration. You can also view the data in other packages such as FSL or AFNI. If you followed the steps above, you likely won't see many bright spots because we only ran the searchlight in a tiny part of the brain. But if you look at the data in fslview and change the coordinates to X=30, Y=31, Z=13 you should see a non-zero value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T-amw4swcSu",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Executing on multiple subjects <a id=\"multi_subj\"></a>\n",
        "\n",
        "So far we have loaded data for one subject and performed within-subject analysis, similar to what we have done in notebooks 1-5 (but in standard MNI space). You can also perform searchlight analysis on a group of subjects. This requires making a list of subjects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UZwSZSab_9z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a2a71804-ca54-4c7b-b930-6f08fcc09329"
      },
      "source": [
        "path = '/content/'\n",
        "all_bold_vol = np.load(path+'bold_data_games.npy')\n",
        "# bold_vol = all_bold_vol[:,:,0]\n",
        "\n",
        "# load mask and get voxel coordinates\n",
        "mask_arr = np.load(path+'mask_arr.npy') # all masks are the same\n",
        "mask_mat = mask_arr[0] # so we can pick any one from the array\n",
        "coords_mat = np.array(np.where(mask_mat == 1)) # so need one set of voxel coordinates for all\n",
        "coords_mat[[0, 2]] = coords_mat[[2, 0]] # exchange the rows\n",
        "print(coords_mat.shape) #coords_mat contains voxel coordinates of brain region voxels\n",
        "\n",
        "# mask_nii is the functional mask, this selects the brain voxels\n",
        "mask_nii = nib.load(os.path.join(path, 'mask.nii')) \n",
        "print(mask_nii.shape)\n",
        "# we get the brain mask (boolean array) with the .dataobj method\n",
        "# brain_mask contains all voxels, 1 at brain regions\n",
        "# coords_mat can be used to index into brain_mask \n",
        "brain_mask = np.array(mask_nii.dataobj)\n",
        "print(brain_mask.shape)\n",
        "affine_mat = mask_nii.affine\n",
        "dimsize = mask_nii.header.get_zooms()\n",
        "\n",
        "# Get the list of nonzero voxel coordinates from the nii mask. SAME AS coords_mat\n",
        "coords_nii = np.where(brain_mask)\n",
        "print(coords_nii[0])\n",
        "print(coords_mat[0])\n",
        "# cords_nii corresponds to the bold_vol <=> verify with Daphne\n",
        "\n",
        "# this where we plot our mask ON (sometimes called brain_nii) - the anatomical/structural image\n",
        "mean_nii = nib.load(os.path.join(path, 'mean.nii')) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 220075)\n",
            "(79, 95, 79)\n",
            "(79, 95, 79)\n",
            "[ 2  3  3 ... 74 74 74]\n",
            "[39 40 36 ... 34 35 44]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ5Abwm2yj_D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "1a1b65a6-5bb8-4e27-f1a4-d71f575dbce5"
      },
      "source": [
        "behavior_RDM = np.load(path+'behavior_RDM.npy')\n",
        "print(behavior_RDM)\n",
        "behavior_RDM = np.tril(behavior_RDM,-1).ravel()\n",
        "behavior_RDM = behavior_RDM[behavior_RDM != 0]\n",
        "print(behavior_RDM)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.   0.59 0.55 0.51 0.47 0.36]\n",
            " [0.59 0.   0.63 0.51 0.51 0.51]\n",
            " [0.55 0.63 0.   0.57 0.47 0.47]\n",
            " [0.51 0.51 0.57 0.   0.5  0.43]\n",
            " [0.47 0.51 0.47 0.5  0.   0.64]\n",
            " [0.36 0.51 0.47 0.43 0.64 0.  ]]\n",
            "[0.59 0.55 0.63 0.51 0.51 0.57 0.47 0.51 0.47 0.5  0.36 0.51 0.47 0.43\n",
            " 0.64]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT-mC3Dg-_j7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e0c5b459-c270-4262-eaf5-ef7826ebbf2d"
      },
      "source": [
        "coords = tuple(coords_mat)\n",
        "small_mask = np.zeros(brain_mask.shape)\n",
        "small_mask[coords] = 1\n",
        "# small_mask[42, 28, 26] = 1\n",
        "print(np.where(small_mask))\n",
        "vol4D = mask_nii.shape+(6,)\n",
        "\n",
        "# For 8 subjects\n",
        "all_bold = []\n",
        "for sub_id in range(8):\n",
        "    isc_vol = np.zeros(vol4D)\n",
        "    bold_vol = all_bold_vol[:,:,sub_id]\n",
        "    for i in range(6):\n",
        "        for j in range(len(coords[0])):\n",
        "            isc_vol[(coords[0][j], coords[1][j], coords[2][j], i)] = bold_vol[i][j]\n",
        "    all_bold.append(isc_vol)\n",
        "\n",
        "# Preset the variables\n",
        "data = all_bold\n",
        "mask = small_mask\n",
        "bcvar = behavior_RDM\n",
        "sl_rad = 5\n",
        "max_blk_edge = 5\n",
        "pool_size = 1\n",
        "\n",
        "\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([ 2,  3,  3, ..., 74, 74, 74]), array([43, 42, 43, ..., 45, 47, 51]), array([32, 31, 32, ..., 31, 26, 26]))\n",
            "Setup searchlight inputs\n",
            "Number of subjects: 8\n",
            "Input data shape: (79, 95, 79, 6)\n",
            "Input mask shape: (79, 95, 79)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCiwDyo7Y9k7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c03ceee1-fbd4-43e3-a257-148bd8cf39cc"
      },
      "source": [
        "sl_rad = 3\n",
        "# Start the clock to time searchlight\n",
        "begin_time = time.time()\n",
        "\n",
        "# Create the searchlight object\n",
        "sl = Searchlight(sl_rad=sl_rad,max_blk_edge=max_blk_edge)\n",
        "print(\"Setup searchlight inputs\")\n",
        "print(\"Number of subjects: \" + str(len(data)))\n",
        "print(\"Input data shape: \" + str(data[0].shape))\n",
        "print(\"Input mask shape: \" + str(mask.shape) + \"\\n\")\n",
        "\n",
        "# Distribute the information to the searchlights (preparing it to run)\n",
        "sl.distribute(data, mask)\n",
        "# Data that is needed for all searchlights is sent to all cores via the sl.broadcast function. \n",
        "#In this example, we are sending the labels for classification to all searchlights.\n",
        "sl.broadcast(bcvar)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setup searchlight inputs\n",
            "Number of subjects: 8\n",
            "Input data shape: (79, 95, 79, 6)\n",
            "Input mask shape: (79, 95, 79)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIt6gmZSC8bq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "93d20a84-e921-4bae-bf66-a3b006727039"
      },
      "source": [
        "\n",
        "# Set up the kernel, RDM analysis\n",
        "def rdm_all(data, sl_mask, myrad, bcvar):\n",
        "    all_rho = []\n",
        "    behavior_RDM = bcvar\n",
        "    # Loop over subject: \n",
        "    for idx in range(len(data)):\n",
        "        data4D = data[idx]\n",
        "        bolddata_sl = data4D.reshape(sl_mask.shape[0] * sl_mask.shape[1] * sl_mask.shape[2], data[0].shape[3]).T\n",
        "        neural_RDM = np.tril(1-np.corrcoef(bolddata_sl), -1)\n",
        "        neural_RDM = neural_RDM.ravel()\n",
        "        neural_RDM = neural_RDM[neural_RDM != 0]\n",
        "        subject_spearman = scipy.stats.spearmanr(neural_RDM, behavior_RDM,axis=None)\n",
        "        all_rho.append(subject_spearman.correlation)\n",
        "    tstats,p = scipy.stats.ttest_1samp(np.arctanh(all_rho), popmean=0)\n",
        "    return (tstats, p)\n",
        "\n",
        "# Execute searchlight on 8 subjects\n",
        "print(\"Begin Searchlight\\n\")\n",
        "sl_result_allsubj = sl.run_searchlight(rdm_all, pool_size=pool_size)\n",
        "print(\"End Searchlight\\n\")\n",
        "print(sl_result_allsubj[mask==1])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Begin Searchlight\n",
            "\n",
            "End Searchlight\n",
            "\n",
            "[None (-0.10951345066840651, 0.9158686517461855)\n",
            " (-0.36733116315278297, 0.7242260808710574) ...\n",
            " (0.7420961754191298, 0.4821812437603141)\n",
            " (-0.055390022926200745, 0.9573755369719533)\n",
            " (0.09709537592823557, 0.9253722009805174)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s4qnZCAFuLB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0556008c-d747-4802-d73d-1a7a39ff8544"
      },
      "source": [
        "print(sl_result_allsubj.shape, sl_result_allsubj[coords].shape)\n",
        "print(sl_result_allsubj[coords])\n",
        "brain_results = sl_result_allsubj[coords]\n",
        "all_rho = []\n",
        "all_pvalues = []\n",
        "for i in range(len(brain_results)):\n",
        "    if brain_results[i] is not None:\n",
        "        all_rho.append(brain_results[i][0].astype('double'))\n",
        "        all_pvalues.append(brain_results[i][1].astype('double'))\n",
        "    else:\n",
        "        all_rho.append(0)\n",
        "        all_pvalues.append(1)\n",
        "print(len(all_rho))\n",
        "print(len(all_pvalues))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(79, 95, 79) (220075,)\n",
            "[None None None ... None None None]\n",
            "220075\n",
            "220075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od5YZpLHTBWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "np.save(path+'pvalues_all', all_pvalues)\n",
        "np.save(path+'tstats_all', all_rho)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viC6vE3w6Tos",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9778d5af-d354-421a-aba7-031f1692f72d"
      },
      "source": [
        "from nilearn import plotting\n",
        "# Save the results to a .nii file\n",
        "# output_name = os.path.join(path, ('allsubj_result.nii.gz'))\n",
        "# sl_nii = nib.Nifti1Image(all_rho, affine_mat, mask_nii.header)  # create the volume image\n",
        "# hdr = sl_nii.header  # get a handle of the .nii file's header\n",
        "# hdr.set_zooms((dimsize[0], dimsize[1], dimsize[2]))\n",
        "# nib.save(sl_nii, output_name)  # Save the volume\n",
        "tstats=all_rho\n",
        "pvalues=np.array(all_pvalues).transpose()\n",
        "brain_nii=mean_nii\n",
        "theta=0.1\n",
        "cut_coords=[36, -44, -16]\n",
        "vmax=12\n",
        "# 1) create a volume from mask_nii\n",
        "isc_vol = np.zeros(mask_nii.shape) \n",
        "\n",
        "print(f'Display t statistics with a corresponding p < {theta}')\n",
        "\n",
        "# use theta to select the t statistics to plot\n",
        "# make arr to store the t stats with p < theta\n",
        "sig_arr = np.zeros(pvalues.shape)\n",
        "\n",
        "# get p indices smaller or equal to theta\n",
        "theta_indices = np.where(pvalues < theta)\n",
        "print(theta_indices[0])\n",
        "\n",
        "# get the t stats at these indices\n",
        "selected_tstats = np.array(tstats)[theta_indices[0]\n",
        "                                   ]\n",
        "\n",
        "# map the selected t statistics on the right places\n",
        "sig_arr[theta_indices] = selected_tstats\n",
        "\n",
        "# 2) Map the t statistics in voxel space\n",
        "isc_vol[coords] = sig_arr\n",
        "\n",
        "# 3) Create a nifti image from this with the affine from mask_nii\n",
        "isc_nifti = nib.Nifti1Image(isc_vol, mask_nii.affine, mask_nii.header)\n",
        "\n",
        "f, ax = plt.subplots(1,1, figsize = (12, 5), dpi=90)\n",
        "plotting.plot_stat_map(\n",
        "    stat_map_img=isc_nifti, \n",
        "    bg_img=brain_nii,\n",
        "    axes=ax,\n",
        "    black_bg=True,\n",
        "    cut_coords=cut_coords,\n",
        "    vmax=vmax\n",
        ");\n",
        "print(\"here\")\n",
        "ax.set_title(f't map with threshold p < {theta}'); \n",
        "plt.show()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Display t statistics with a corresponding p < 0.1\n",
            "[   723    737    766 ... 218177 218207 218485]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "/* Put everything inside the global mpl namespace */\n",
              "/* global mpl */\n",
              "window.mpl = {};\n",
              "\n",
              "mpl.get_websocket_type = function () {\n",
              "    if (typeof WebSocket !== 'undefined') {\n",
              "        return WebSocket;\n",
              "    } else if (typeof MozWebSocket !== 'undefined') {\n",
              "        return MozWebSocket;\n",
              "    } else {\n",
              "        alert(\n",
              "            'Your browser does not have WebSocket support. ' +\n",
              "                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
              "                'Firefox 4 and 5 are also supported but you ' +\n",
              "                'have to enable WebSockets in about:config.'\n",
              "        );\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n",
              "    this.id = figure_id;\n",
              "\n",
              "    this.ws = websocket;\n",
              "\n",
              "    this.supports_binary = this.ws.binaryType !== undefined;\n",
              "\n",
              "    if (!this.supports_binary) {\n",
              "        var warnings = document.getElementById('mpl-warnings');\n",
              "        if (warnings) {\n",
              "            warnings.style.display = 'block';\n",
              "            warnings.textContent =\n",
              "                'This browser does not support binary websocket messages. ' +\n",
              "                'Performance may be slow.';\n",
              "        }\n",
              "    }\n",
              "\n",
              "    this.imageObj = new Image();\n",
              "\n",
              "    this.context = undefined;\n",
              "    this.message = undefined;\n",
              "    this.canvas = undefined;\n",
              "    this.rubberband_canvas = undefined;\n",
              "    this.rubberband_context = undefined;\n",
              "    this.format_dropdown = undefined;\n",
              "\n",
              "    this.image_mode = 'full';\n",
              "\n",
              "    this.root = document.createElement('div');\n",
              "    this.root.setAttribute('style', 'display: inline-block');\n",
              "    this._root_extra_style(this.root);\n",
              "\n",
              "    parent_element.appendChild(this.root);\n",
              "\n",
              "    this._init_header(this);\n",
              "    this._init_canvas(this);\n",
              "    this._init_toolbar(this);\n",
              "\n",
              "    var fig = this;\n",
              "\n",
              "    this.waiting = false;\n",
              "\n",
              "    this.ws.onopen = function () {\n",
              "        fig.send_message('supports_binary', { value: fig.supports_binary });\n",
              "        fig.send_message('send_image_mode', {});\n",
              "        if (mpl.ratio !== 1) {\n",
              "            fig.send_message('set_dpi_ratio', { dpi_ratio: mpl.ratio });\n",
              "        }\n",
              "        fig.send_message('refresh', {});\n",
              "    };\n",
              "\n",
              "    this.imageObj.onload = function () {\n",
              "        if (fig.image_mode === 'full') {\n",
              "            // Full images could contain transparency (where diff images\n",
              "            // almost always do), so we need to clear the canvas so that\n",
              "            // there is no ghosting.\n",
              "            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
              "        }\n",
              "        fig.context.drawImage(fig.imageObj, 0, 0);\n",
              "    };\n",
              "\n",
              "    this.imageObj.onunload = function () {\n",
              "        fig.ws.close();\n",
              "    };\n",
              "\n",
              "    this.ws.onmessage = this._make_on_message_function(this);\n",
              "\n",
              "    this.ondownload = ondownload;\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._init_header = function () {\n",
              "    var titlebar = document.createElement('div');\n",
              "    titlebar.classList =\n",
              "        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n",
              "    var titletext = document.createElement('div');\n",
              "    titletext.classList = 'ui-dialog-title';\n",
              "    titletext.setAttribute(\n",
              "        'style',\n",
              "        'width: 100%; text-align: center; padding: 3px;'\n",
              "    );\n",
              "    titlebar.appendChild(titletext);\n",
              "    this.root.appendChild(titlebar);\n",
              "    this.header = titletext;\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n",
              "\n",
              "mpl.figure.prototype._init_canvas = function () {\n",
              "    var fig = this;\n",
              "\n",
              "    var canvas_div = (this.canvas_div = document.createElement('div'));\n",
              "    canvas_div.setAttribute(\n",
              "        'style',\n",
              "        'border: 1px solid #ddd;' +\n",
              "            'box-sizing: content-box;' +\n",
              "            'clear: both;' +\n",
              "            'min-height: 1px;' +\n",
              "            'min-width: 1px;' +\n",
              "            'outline: 0;' +\n",
              "            'overflow: hidden;' +\n",
              "            'position: relative;' +\n",
              "            'resize: both;'\n",
              "    );\n",
              "\n",
              "    function on_keyboard_event_closure(name) {\n",
              "        return function (event) {\n",
              "            return fig.key_event(event, name);\n",
              "        };\n",
              "    }\n",
              "\n",
              "    canvas_div.addEventListener(\n",
              "        'keydown',\n",
              "        on_keyboard_event_closure('key_press')\n",
              "    );\n",
              "    canvas_div.addEventListener(\n",
              "        'keyup',\n",
              "        on_keyboard_event_closure('key_release')\n",
              "    );\n",
              "\n",
              "    this._canvas_extra_style(canvas_div);\n",
              "    this.root.appendChild(canvas_div);\n",
              "\n",
              "    var canvas = (this.canvas = document.createElement('canvas'));\n",
              "    canvas.classList.add('mpl-canvas');\n",
              "    canvas.setAttribute('style', 'box-sizing: content-box;');\n",
              "\n",
              "    this.context = canvas.getContext('2d');\n",
              "\n",
              "    var backingStore =\n",
              "        this.context.backingStorePixelRatio ||\n",
              "        this.context.webkitBackingStorePixelRatio ||\n",
              "        this.context.mozBackingStorePixelRatio ||\n",
              "        this.context.msBackingStorePixelRatio ||\n",
              "        this.context.oBackingStorePixelRatio ||\n",
              "        this.context.backingStorePixelRatio ||\n",
              "        1;\n",
              "\n",
              "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
              "\n",
              "    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n",
              "        'canvas'\n",
              "    ));\n",
              "    rubberband_canvas.setAttribute(\n",
              "        'style',\n",
              "        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'\n",
              "    );\n",
              "\n",
              "    var resizeObserver = new ResizeObserver(function (entries) {\n",
              "        var nentries = entries.length;\n",
              "        for (var i = 0; i < nentries; i++) {\n",
              "            var entry = entries[i];\n",
              "            var width, height;\n",
              "            if (entry.contentBoxSize) {\n",
              "                width = entry.contentBoxSize.inlineSize;\n",
              "                height = entry.contentBoxSize.blockSize;\n",
              "            } else {\n",
              "                width = entry.contentRect.width;\n",
              "                height = entry.contentRect.height;\n",
              "            }\n",
              "\n",
              "            // Keep the size of the canvas and rubber band canvas in sync with\n",
              "            // the canvas container.\n",
              "            canvas.setAttribute('width', width * mpl.ratio);\n",
              "            canvas.setAttribute('height', height * mpl.ratio);\n",
              "            canvas.setAttribute(\n",
              "                'style',\n",
              "                'width: ' + width + 'px; height: ' + height + 'px;'\n",
              "            );\n",
              "\n",
              "            rubberband_canvas.setAttribute('width', width);\n",
              "            rubberband_canvas.setAttribute('height', height);\n",
              "\n",
              "            // And update the size in Python. We ignore the initial 0/0 size\n",
              "            // that occurs as the element is placed into the DOM, which should\n",
              "            // otherwise not happen due to the minimum size styling.\n",
              "            if (width != 0 && height != 0) {\n",
              "                fig.request_resize(width, height);\n",
              "            }\n",
              "        }\n",
              "    });\n",
              "    resizeObserver.observe(canvas_div);\n",
              "\n",
              "    function on_mouse_event_closure(name) {\n",
              "        return function (event) {\n",
              "            return fig.mouse_event(event, name);\n",
              "        };\n",
              "    }\n",
              "\n",
              "    rubberband_canvas.addEventListener(\n",
              "        'mousedown',\n",
              "        on_mouse_event_closure('button_press')\n",
              "    );\n",
              "    rubberband_canvas.addEventListener(\n",
              "        'mouseup',\n",
              "        on_mouse_event_closure('button_release')\n",
              "    );\n",
              "    // Throttle sequential mouse events to 1 every 20ms.\n",
              "    rubberband_canvas.addEventListener(\n",
              "        'mousemove',\n",
              "        on_mouse_event_closure('motion_notify')\n",
              "    );\n",
              "\n",
              "    rubberband_canvas.addEventListener(\n",
              "        'mouseenter',\n",
              "        on_mouse_event_closure('figure_enter')\n",
              "    );\n",
              "    rubberband_canvas.addEventListener(\n",
              "        'mouseleave',\n",
              "        on_mouse_event_closure('figure_leave')\n",
              "    );\n",
              "\n",
              "    canvas_div.addEventListener('wheel', function (event) {\n",
              "        if (event.deltaY < 0) {\n",
              "            event.step = 1;\n",
              "        } else {\n",
              "            event.step = -1;\n",
              "        }\n",
              "        on_mouse_event_closure('scroll')(event);\n",
              "    });\n",
              "\n",
              "    canvas_div.appendChild(canvas);\n",
              "    canvas_div.appendChild(rubberband_canvas);\n",
              "\n",
              "    this.rubberband_context = rubberband_canvas.getContext('2d');\n",
              "    this.rubberband_context.strokeStyle = '#000000';\n",
              "\n",
              "    this._resize_canvas = function (width, height, forward) {\n",
              "        if (forward) {\n",
              "            canvas_div.style.width = width + 'px';\n",
              "            canvas_div.style.height = height + 'px';\n",
              "        }\n",
              "    };\n",
              "\n",
              "    // Disable right mouse context menu.\n",
              "    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {\n",
              "        return false;\n",
              "    });\n",
              "\n",
              "    function set_focus() {\n",
              "        canvas.focus();\n",
              "        canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    window.setTimeout(set_focus, 100);\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function () {\n",
              "    var fig = this;\n",
              "\n",
              "    var toolbar = document.createElement('div');\n",
              "    toolbar.classList = 'mpl-toolbar';\n",
              "    this.root.appendChild(toolbar);\n",
              "\n",
              "    function on_click_closure(name) {\n",
              "        return function (_event) {\n",
              "            return fig.toolbar_button_onclick(name);\n",
              "        };\n",
              "    }\n",
              "\n",
              "    function on_mouseover_closure(tooltip) {\n",
              "        return function (event) {\n",
              "            if (!event.currentTarget.disabled) {\n",
              "                return fig.toolbar_button_onmouseover(tooltip);\n",
              "            }\n",
              "        };\n",
              "    }\n",
              "\n",
              "    fig.buttons = {};\n",
              "    var buttonGroup = document.createElement('div');\n",
              "    buttonGroup.classList = 'mpl-button-group';\n",
              "    for (var toolbar_ind in mpl.toolbar_items) {\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) {\n",
              "            /* Instead of a spacer, we start a new button group. */\n",
              "            if (buttonGroup.hasChildNodes()) {\n",
              "                toolbar.appendChild(buttonGroup);\n",
              "            }\n",
              "            buttonGroup = document.createElement('div');\n",
              "            buttonGroup.classList = 'mpl-button-group';\n",
              "            continue;\n",
              "        }\n",
              "\n",
              "        var button = (fig.buttons[name] = document.createElement('button'));\n",
              "        button.classList = 'mpl-widget';\n",
              "        button.setAttribute('role', 'button');\n",
              "        button.setAttribute('aria-disabled', 'false');\n",
              "        button.addEventListener('click', on_click_closure(method_name));\n",
              "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
              "\n",
              "        var icon_img = document.createElement('img');\n",
              "        icon_img.src = '_images/' + image + '.png';\n",
              "        icon_img.srcset = '_images/' + image + '_large.png 2x';\n",
              "        icon_img.alt = tooltip;\n",
              "        button.appendChild(icon_img);\n",
              "\n",
              "        buttonGroup.appendChild(button);\n",
              "    }\n",
              "\n",
              "    if (buttonGroup.hasChildNodes()) {\n",
              "        toolbar.appendChild(buttonGroup);\n",
              "    }\n",
              "\n",
              "    var fmt_picker = document.createElement('select');\n",
              "    fmt_picker.classList = 'mpl-widget';\n",
              "    toolbar.appendChild(fmt_picker);\n",
              "    this.format_dropdown = fmt_picker;\n",
              "\n",
              "    for (var ind in mpl.extensions) {\n",
              "        var fmt = mpl.extensions[ind];\n",
              "        var option = document.createElement('option');\n",
              "        option.selected = fmt === mpl.default_extension;\n",
              "        option.innerHTML = fmt;\n",
              "        fmt_picker.appendChild(option);\n",
              "    }\n",
              "\n",
              "    var status_bar = document.createElement('span');\n",
              "    status_bar.classList = 'mpl-message';\n",
              "    toolbar.appendChild(status_bar);\n",
              "    this.message = status_bar;\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n",
              "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
              "    // which will in turn request a refresh of the image.\n",
              "    this.send_message('resize', { width: x_pixels, height: y_pixels });\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.send_message = function (type, properties) {\n",
              "    properties['type'] = type;\n",
              "    properties['figure_id'] = this.id;\n",
              "    this.ws.send(JSON.stringify(properties));\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.send_draw_message = function () {\n",
              "    if (!this.waiting) {\n",
              "        this.waiting = true;\n",
              "        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
              "    var format_dropdown = fig.format_dropdown;\n",
              "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
              "    fig.ondownload(fig, format);\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_resize = function (fig, msg) {\n",
              "    var size = msg['size'];\n",
              "    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n",
              "        fig._resize_canvas(size[0], size[1], msg['forward']);\n",
              "        fig.send_message('refresh', {});\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n",
              "    var x0 = msg['x0'] / mpl.ratio;\n",
              "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
              "    var x1 = msg['x1'] / mpl.ratio;\n",
              "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
              "    x0 = Math.floor(x0) + 0.5;\n",
              "    y0 = Math.floor(y0) + 0.5;\n",
              "    x1 = Math.floor(x1) + 0.5;\n",
              "    y1 = Math.floor(y1) + 0.5;\n",
              "    var min_x = Math.min(x0, x1);\n",
              "    var min_y = Math.min(y0, y1);\n",
              "    var width = Math.abs(x1 - x0);\n",
              "    var height = Math.abs(y1 - y0);\n",
              "\n",
              "    fig.rubberband_context.clearRect(\n",
              "        0,\n",
              "        0,\n",
              "        fig.canvas.width / mpl.ratio,\n",
              "        fig.canvas.height / mpl.ratio\n",
              "    );\n",
              "\n",
              "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n",
              "    // Updates the figure title.\n",
              "    fig.header.textContent = msg['label'];\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_cursor = function (fig, msg) {\n",
              "    var cursor = msg['cursor'];\n",
              "    switch (cursor) {\n",
              "        case 0:\n",
              "            cursor = 'pointer';\n",
              "            break;\n",
              "        case 1:\n",
              "            cursor = 'default';\n",
              "            break;\n",
              "        case 2:\n",
              "            cursor = 'crosshair';\n",
              "            break;\n",
              "        case 3:\n",
              "            cursor = 'move';\n",
              "            break;\n",
              "    }\n",
              "    fig.rubberband_canvas.style.cursor = cursor;\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_message = function (fig, msg) {\n",
              "    fig.message.textContent = msg['message'];\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_draw = function (fig, _msg) {\n",
              "    // Request the server to send over a new figure.\n",
              "    fig.send_draw_message();\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n",
              "    fig.image_mode = msg['mode'];\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n",
              "    for (var key in msg) {\n",
              "        if (!(key in fig.buttons)) {\n",
              "            continue;\n",
              "        }\n",
              "        fig.buttons[key].disabled = !msg[key];\n",
              "        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n",
              "    if (msg['mode'] === 'PAN') {\n",
              "        fig.buttons['Pan'].classList.add('active');\n",
              "        fig.buttons['Zoom'].classList.remove('active');\n",
              "    } else if (msg['mode'] === 'ZOOM') {\n",
              "        fig.buttons['Pan'].classList.remove('active');\n",
              "        fig.buttons['Zoom'].classList.add('active');\n",
              "    } else {\n",
              "        fig.buttons['Pan'].classList.remove('active');\n",
              "        fig.buttons['Zoom'].classList.remove('active');\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function () {\n",
              "    // Called whenever the canvas gets updated.\n",
              "    this.send_message('ack', {});\n",
              "};\n",
              "\n",
              "// A function to construct a web socket function for onmessage handling.\n",
              "// Called in the figure constructor.\n",
              "mpl.figure.prototype._make_on_message_function = function (fig) {\n",
              "    return function socket_on_message(evt) {\n",
              "        if (evt.data instanceof Blob) {\n",
              "            /* FIXME: We get \"Resource interpreted as Image but\n",
              "             * transferred with MIME type text/plain:\" errors on\n",
              "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
              "             * to be part of the websocket stream */\n",
              "            evt.data.type = 'image/png';\n",
              "\n",
              "            /* Free the memory for the previous frames */\n",
              "            if (fig.imageObj.src) {\n",
              "                (window.URL || window.webkitURL).revokeObjectURL(\n",
              "                    fig.imageObj.src\n",
              "                );\n",
              "            }\n",
              "\n",
              "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
              "                evt.data\n",
              "            );\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        } else if (\n",
              "            typeof evt.data === 'string' &&\n",
              "            evt.data.slice(0, 21) === 'data:image/png;base64'\n",
              "        ) {\n",
              "            fig.imageObj.src = evt.data;\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        var msg = JSON.parse(evt.data);\n",
              "        var msg_type = msg['type'];\n",
              "\n",
              "        // Call the  \"handle_{type}\" callback, which takes\n",
              "        // the figure and JSON message as its only arguments.\n",
              "        try {\n",
              "            var callback = fig['handle_' + msg_type];\n",
              "        } catch (e) {\n",
              "            console.log(\n",
              "                \"No handler for the '\" + msg_type + \"' message type: \",\n",
              "                msg\n",
              "            );\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        if (callback) {\n",
              "            try {\n",
              "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
              "                callback(fig, msg);\n",
              "            } catch (e) {\n",
              "                console.log(\n",
              "                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n",
              "                    e,\n",
              "                    e.stack,\n",
              "                    msg\n",
              "                );\n",
              "            }\n",
              "        }\n",
              "    };\n",
              "};\n",
              "\n",
              "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
              "mpl.findpos = function (e) {\n",
              "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
              "    var targ;\n",
              "    if (!e) {\n",
              "        e = window.event;\n",
              "    }\n",
              "    if (e.target) {\n",
              "        targ = e.target;\n",
              "    } else if (e.srcElement) {\n",
              "        targ = e.srcElement;\n",
              "    }\n",
              "    if (targ.nodeType === 3) {\n",
              "        // defeat Safari bug\n",
              "        targ = targ.parentNode;\n",
              "    }\n",
              "\n",
              "    // pageX,Y are the mouse positions relative to the document\n",
              "    var boundingRect = targ.getBoundingClientRect();\n",
              "    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);\n",
              "    var y = e.pageY - (boundingRect.top + document.body.scrollTop);\n",
              "\n",
              "    return { x: x, y: y };\n",
              "};\n",
              "\n",
              "/*\n",
              " * return a copy of an object with only non-object keys\n",
              " * we need this to avoid circular references\n",
              " * http://stackoverflow.com/a/24161582/3208463\n",
              " */\n",
              "function simpleKeys(original) {\n",
              "    return Object.keys(original).reduce(function (obj, key) {\n",
              "        if (typeof original[key] !== 'object') {\n",
              "            obj[key] = original[key];\n",
              "        }\n",
              "        return obj;\n",
              "    }, {});\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.mouse_event = function (event, name) {\n",
              "    var canvas_pos = mpl.findpos(event);\n",
              "\n",
              "    if (name === 'button_press') {\n",
              "        this.canvas.focus();\n",
              "        this.canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    var x = canvas_pos.x * mpl.ratio;\n",
              "    var y = canvas_pos.y * mpl.ratio;\n",
              "\n",
              "    this.send_message(name, {\n",
              "        x: x,\n",
              "        y: y,\n",
              "        button: event.button,\n",
              "        step: event.step,\n",
              "        guiEvent: simpleKeys(event),\n",
              "    });\n",
              "\n",
              "    /* This prevents the web browser from automatically changing to\n",
              "     * the text insertion cursor when the button is pressed.  We want\n",
              "     * to control all of the cursor setting manually through the\n",
              "     * 'cursor' event from matplotlib */\n",
              "    event.preventDefault();\n",
              "    return false;\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function (_event, _name) {\n",
              "    // Handle any extra behaviour associated with a key event\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.key_event = function (event, name) {\n",
              "    // Prevent repeat events\n",
              "    if (name === 'key_press') {\n",
              "        if (event.which === this._key) {\n",
              "            return;\n",
              "        } else {\n",
              "            this._key = event.which;\n",
              "        }\n",
              "    }\n",
              "    if (name === 'key_release') {\n",
              "        this._key = null;\n",
              "    }\n",
              "\n",
              "    var value = '';\n",
              "    if (event.ctrlKey && event.which !== 17) {\n",
              "        value += 'ctrl+';\n",
              "    }\n",
              "    if (event.altKey && event.which !== 18) {\n",
              "        value += 'alt+';\n",
              "    }\n",
              "    if (event.shiftKey && event.which !== 16) {\n",
              "        value += 'shift+';\n",
              "    }\n",
              "\n",
              "    value += 'k';\n",
              "    value += event.which.toString();\n",
              "\n",
              "    this._key_event_extra(event, name);\n",
              "\n",
              "    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n",
              "    return false;\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onclick = function (name) {\n",
              "    if (name === 'download') {\n",
              "        this.handle_save(this, null);\n",
              "    } else {\n",
              "        this.send_message('toolbar_button', { name: name });\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n",
              "    this.message.textContent = tooltip;\n",
              "};\n",
              "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
              "\n",
              "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
              "\n",
              "mpl.default_extension = \"png\";/* global mpl */\n",
              "\n",
              "var comm_websocket_adapter = function (comm) {\n",
              "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
              "    // object with the appropriate methods. Currently this is a non binary\n",
              "    // socket, so there is still some room for performance tuning.\n",
              "    var ws = {};\n",
              "\n",
              "    ws.close = function () {\n",
              "        comm.close();\n",
              "    };\n",
              "    ws.send = function (m) {\n",
              "        //console.log('sending', m);\n",
              "        comm.send(m);\n",
              "    };\n",
              "    // Register the callback with on_msg.\n",
              "    comm.on_msg(function (msg) {\n",
              "        //console.log('receiving', msg['content']['data'], msg);\n",
              "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
              "        ws.onmessage(msg['content']['data']);\n",
              "    });\n",
              "    return ws;\n",
              "};\n",
              "\n",
              "mpl.mpl_figure_comm = function (comm, msg) {\n",
              "    // This is the function which gets called when the mpl process\n",
              "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
              "\n",
              "    var id = msg.content.data.id;\n",
              "    // Get hold of the div created by the display call when the Comm\n",
              "    // socket was opened in Python.\n",
              "    var element = document.getElementById(id);\n",
              "    var ws_proxy = comm_websocket_adapter(comm);\n",
              "\n",
              "    function ondownload(figure, _format) {\n",
              "        window.open(figure.canvas.toDataURL());\n",
              "    }\n",
              "\n",
              "    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n",
              "\n",
              "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
              "    // web socket which is closed, not our websocket->open comm proxy.\n",
              "    ws_proxy.onopen();\n",
              "\n",
              "    fig.parent_element = element;\n",
              "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
              "    if (!fig.cell_info) {\n",
              "        console.error('Failed to find cell for figure', id, fig);\n",
              "        return;\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_close = function (fig, msg) {\n",
              "    var width = fig.canvas.width / mpl.ratio;\n",
              "    fig.root.removeEventListener('remove', this._remove_fig_handler);\n",
              "\n",
              "    // Update the output cell to use the data from the current canvas.\n",
              "    fig.push_to_output();\n",
              "    var dataURL = fig.canvas.toDataURL();\n",
              "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
              "    // the notebook keyboard shortcuts fail.\n",
              "    IPython.keyboard_manager.enable();\n",
              "    fig.parent_element.innerHTML =\n",
              "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
              "    fig.close_ws(fig, msg);\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.close_ws = function (fig, msg) {\n",
              "    fig.send_message('closing', msg);\n",
              "    // fig.ws.close()\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n",
              "    // Turn the data on the canvas into data in the output cell.\n",
              "    var width = this.canvas.width / mpl.ratio;\n",
              "    var dataURL = this.canvas.toDataURL();\n",
              "    this.cell_info[1]['text/html'] =\n",
              "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function () {\n",
              "    // Tell IPython that the notebook contents must change.\n",
              "    IPython.notebook.set_dirty(true);\n",
              "    this.send_message('ack', {});\n",
              "    var fig = this;\n",
              "    // Wait a second, then push the new image to the DOM so\n",
              "    // that it is saved nicely (might be nice to debounce this).\n",
              "    setTimeout(function () {\n",
              "        fig.push_to_output();\n",
              "    }, 1000);\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function () {\n",
              "    var fig = this;\n",
              "\n",
              "    var toolbar = document.createElement('div');\n",
              "    toolbar.classList = 'btn-toolbar';\n",
              "    this.root.appendChild(toolbar);\n",
              "\n",
              "    function on_click_closure(name) {\n",
              "        return function (_event) {\n",
              "            return fig.toolbar_button_onclick(name);\n",
              "        };\n",
              "    }\n",
              "\n",
              "    function on_mouseover_closure(tooltip) {\n",
              "        return function (event) {\n",
              "            if (!event.currentTarget.disabled) {\n",
              "                return fig.toolbar_button_onmouseover(tooltip);\n",
              "            }\n",
              "        };\n",
              "    }\n",
              "\n",
              "    fig.buttons = {};\n",
              "    var buttonGroup = document.createElement('div');\n",
              "    buttonGroup.classList = 'btn-group';\n",
              "    var button;\n",
              "    for (var toolbar_ind in mpl.toolbar_items) {\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) {\n",
              "            /* Instead of a spacer, we start a new button group. */\n",
              "            if (buttonGroup.hasChildNodes()) {\n",
              "                toolbar.appendChild(buttonGroup);\n",
              "            }\n",
              "            buttonGroup = document.createElement('div');\n",
              "            buttonGroup.classList = 'btn-group';\n",
              "            continue;\n",
              "        }\n",
              "\n",
              "        button = fig.buttons[name] = document.createElement('button');\n",
              "        button.classList = 'btn btn-default';\n",
              "        button.href = '#';\n",
              "        button.title = name;\n",
              "        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n",
              "        button.addEventListener('click', on_click_closure(method_name));\n",
              "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
              "        buttonGroup.appendChild(button);\n",
              "    }\n",
              "\n",
              "    if (buttonGroup.hasChildNodes()) {\n",
              "        toolbar.appendChild(buttonGroup);\n",
              "    }\n",
              "\n",
              "    // Add the status bar.\n",
              "    var status_bar = document.createElement('span');\n",
              "    status_bar.classList = 'mpl-message pull-right';\n",
              "    toolbar.appendChild(status_bar);\n",
              "    this.message = status_bar;\n",
              "\n",
              "    // Add the close button to the window.\n",
              "    var buttongrp = document.createElement('div');\n",
              "    buttongrp.classList = 'btn-group inline pull-right';\n",
              "    button = document.createElement('button');\n",
              "    button.classList = 'btn btn-mini btn-primary';\n",
              "    button.href = '#';\n",
              "    button.title = 'Stop Interaction';\n",
              "    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n",
              "    button.addEventListener('click', function (_evt) {\n",
              "        fig.handle_close(fig, {});\n",
              "    });\n",
              "    button.addEventListener(\n",
              "        'mouseover',\n",
              "        on_mouseover_closure('Stop Interaction')\n",
              "    );\n",
              "    buttongrp.appendChild(button);\n",
              "    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n",
              "    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._remove_fig_handler = function () {\n",
              "    this.close_ws(this, {});\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function (el) {\n",
              "    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n",
              "    el.addEventListener('remove', this._remove_fig_handler);\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function (el) {\n",
              "    // this is important to make the div 'focusable\n",
              "    el.setAttribute('tabindex', 0);\n",
              "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
              "    // off when our div gets focus\n",
              "\n",
              "    // location in version 3\n",
              "    if (IPython.notebook.keyboard_manager) {\n",
              "        IPython.notebook.keyboard_manager.register_events(el);\n",
              "    } else {\n",
              "        // location in version 2\n",
              "        IPython.keyboard_manager.register_events(el);\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function (event, _name) {\n",
              "    var manager = IPython.notebook.keyboard_manager;\n",
              "    if (!manager) {\n",
              "        manager = IPython.keyboard_manager;\n",
              "    }\n",
              "\n",
              "    // Check for shift+enter\n",
              "    if (event.shiftKey && event.which === 13) {\n",
              "        this.canvas_div.blur();\n",
              "        // select the cell after this one\n",
              "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
              "        IPython.notebook.select(index + 1);\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
              "    fig.ondownload(fig, null);\n",
              "};\n",
              "\n",
              "mpl.find_output_cell = function (html_output) {\n",
              "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
              "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
              "    // IPython event is triggered only after the cells have been serialised, which for\n",
              "    // our purposes (turning an active figure into a static one), is too late.\n",
              "    var cells = IPython.notebook.get_cells();\n",
              "    var ncells = cells.length;\n",
              "    for (var i = 0; i < ncells; i++) {\n",
              "        var cell = cells[i];\n",
              "        if (cell.cell_type === 'code') {\n",
              "            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n",
              "                var data = cell.output_area.outputs[j];\n",
              "                if (data.data) {\n",
              "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
              "                    data = data.data;\n",
              "                }\n",
              "                if (data['text/html'] === html_output) {\n",
              "                    return [cell, data, j];\n",
              "                }\n",
              "            }\n",
              "        }\n",
              "    }\n",
              "};\n",
              "\n",
              "// Register the function which deals with the matplotlib target/channel.\n",
              "// The kernel may be null if the page has been refreshed.\n",
              "if (IPython.notebook.kernel !== null) {\n",
              "    IPython.notebook.kernel.comm_manager.register_target(\n",
              "        'matplotlib',\n",
              "        mpl.mpl_figure_comm\n",
              "    );\n",
              "}\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div id='a12fe6be-668f-4227-b82e-5b55efaefe21'></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "here\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgt7MKwAwcSx",
        "colab_type": "text"
      },
      "source": [
        "**Sanity Check:**\n",
        "\n",
        "To make sure that each subject is being processed correctly, we can compare the results by loading one subject alone vs. loading the same subject amongst three subjects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq0UDGr3wcS2",
        "colab_type": "text"
      },
      "source": [
        "## 2. Running searchlight analyses on a cluster<a name=\"submitting_searchlights\"></a>\n",
        "\n",
        "**Note: If you are running this section in a non-cluster environment (e.g., a laptop or a server with limited resources), the run-tine for this section can be quite long. You can make an estimate of the run-time (see [exercise 4](#ex4) above) and plan accordingly.**\n",
        "\n",
        "Running searchlight analyses through notebooks or interactive sessions isn't tractable for real studies. Although the example above ran quickly and without parallelization, we only performed 64 analyses. We are now going to write a script to run a searchlight as a \"batch\" job. To learn how to submit jobs, you need to know a bit about [slurm](https://research.computing.yale.edu/support/hpc/user-guide/slurm), the scheduling system we assume you are using. If you are using a different scheduler you will need to follow different instructions. \n",
        "\n",
        "To run a job, a good work flow is to have two scripts: One script that actually does the computation you care about (e.g., a python script like utils.py) and a bash script that sets up the environment and specifies the job parameters. The environment refers to the modules and packages you need to run your job. The job parameters refer to the partition you are going to use (-p), the number of cores (-n), the amount of memory (-m) and required time (-t). To run your job you then call the bash script with something like: 'sbatch script.sh'\n",
        "\n",
        "**Self-study:** Lucky for you we have already written the script needed here, called `run_searchlight.sh`. This script is written in the bash command language. Please explore this script to get familiar with submitting jobs. It will be very useful for future analyses to customize it for your needs (using a text editor like nano or nedit)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRLY4LdwwcS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the bash script for running searchlights\n",
        "!cat 07-searchlight/run_searchlight.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMGSh0KSwcS5",
        "colab_type": "text"
      },
      "source": [
        "When a job starts, a log file named as ``searchlight-%j.out`` is automatically created in the same directory as your shell script. You can see any output from your script printed into the log file. You can print out the content of the log file in the terminal or open it using a text editor to help debug your code. To check the status of your job you should use `squeue -u $USER`. Sometimes your jobs won't run immediately because they are waiting in the queue. Conversely, don't submit too many jobs or your classmates won't be able to run theirs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4vdA5uvwcS6",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Distributing jobs with parallel computing<a name=\"ranks\"></a>\n",
        "\n",
        "When you parallelize an analysis across multiple cores, this typically means that each core will run the code independently (though there are ways for cores to communicate directly with each other). This means you will load in all of the data on each core. The searchlight function then notices that there are multiple cores running the same job and assigns different pieces of the data to each core for analysis. The message passing interface (MPI), the parallelizing framework used by BrainIAK and most high-performance computing applications, keeps track of each process by assigning it a rank, starting at 0. We provide a brief overview of ranks, cores, and nodes below.\n",
        "\n",
        "<img src=\"https://github.com/brainiak/brainiak-tutorials/blob/master/tutorials/imgs/lab7/nodes_process.jpg?raw=1\" width=\"300\" height=\"300\"/>\n",
        "\n",
        "\n",
        "What is a rank? It is a process generated by the application that you are running. In the above figure \"Process 1\" can be considered to be rank=0 and \"Process 2\" rank=1. A rank can use more than one core on the cluster and is managed by MPI. Each rank also uses a part of the memory allocated. The cores and memory are part of the hardware belonging to a \"node\". A node is akin to a server. When you submit a job, you are requesting use of part of the node, or sometimes even the entire node. **Optimizing the memory, and number of cores for your batch job will lead to significant gains in run-time for your programs**.\n",
        "\n",
        "MPI handles all communication between the ranks (or processes). This can occur within a single node, or if you have asked for lots of resources, this can even span multiple nodes. Each process can spawn multiple threads, and Open-MP handles communication between threads.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/brainiak/brainiak-tutorials/blob/master/tutorials/imgs/lab7/mpi_openmp.jpg?raw=1\" width=\"300\" height=\"300\"/>\n",
        "\n",
        "\n",
        "**You do not need to configure any of the above. BrainIAK handles all the parallelization for you.** Just make sure that you have requested the appropriate number of cores and the memory for your job.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi2veMuAwcS6",
        "colab_type": "text"
      },
      "source": [
        " \n",
        "If you are running an analysis across 2 cores then some of your computations will be run with rank=0 and some with rank=1. The following commands are used to access MPI information that your job needs.\n",
        "\n",
        "```\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.rank\n",
        "size = comm.size\n",
        "```\n",
        "\n",
        "For efficient utilization of memory, we typically load the data on as few ranks as possible. If memory is sufficient to load all data on one rank, then we would write someting like this:\n",
        "```\n",
        "if rank==0:\n",
        "    load all my data\n",
        "else:\n",
        "    load labels for all subject\n",
        "    load the mask\n",
        "distribute(pieces of my data to other ranks)\n",
        "run searchlight\n",
        "```\n",
        "\n",
        "The above will avoid loading every subjects data on every rank. **The labels and mask must be loaded on all ranks.** The distribution will allocate only pieces of the data for processing. Again, this is all handled by MPI. In searchlight, the `sl.distribute` methods handles the distribution to different ranks.\n",
        "\n",
        "After the searchlights have all finished, you need to save the data. MPI gathers the results from all of the cores and BrainIAK saves the results.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxfBQuxfwcS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example of loading data only one rank.\n",
        "from mpi4py import MPI\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.rank\n",
        "size = comm.size\n",
        "\n",
        "sub_id = 1\n",
        "# Load data only on rank==0\n",
        "if rank==0:\n",
        "    bold_vol, labels, whole_brain_mask, affine_mat, dimsize = load_fs_data(data_path, sub_id)\n",
        "    data = bold_vol\n",
        "else:\n",
        "    data = None\n",
        "    \n",
        "# Make a mask of only one arbitrary voxel\n",
        "small_mask = np.zeros(whole_brain_mask.shape)\n",
        "small_mask[28:32, 29:33, 11:15] = 1\n",
        "\n",
        "# Preset the variables\n",
        "\n",
        "mask = small_mask\n",
        "bcvar = labels\n",
        "sl_rad = 1\n",
        "max_blk_edge = 5\n",
        "pool_size = 1\n",
        "\n",
        "\n",
        "# Start the clock to time searchlight\n",
        "begin_time = time.time()\n",
        "\n",
        "# Create the searchlight object\n",
        "sl = Searchlight(sl_rad=sl_rad,max_blk_edge=max_blk_edge)\n",
        "print(\"Setup searchlight inputs\")\n",
        "print(\"Input data shape: \" + str(data.shape))\n",
        "print(\"Input mask shape: \" + str(mask.shape) + \"\\n\")\n",
        "\n",
        "# Distribute the information to the searchlights (preparing it to run)\n",
        "sl.distribute([data], mask)\n",
        "\n",
        "# Data that is needed for all searchlights is sent to all cores via the sl.broadcast function. In this example, we are sending the labels for classification to all searchlights.\n",
        "sl.broadcast(bcvar)\n",
        "\n",
        "# Set up the kernel, in this case an SVM\n",
        "def calc_svm(data, sl_mask, myrad, bcvar):  \n",
        "    if np.sum(sl_mask) < 14:\n",
        "        return -1\n",
        "    # Pull out the data\n",
        "    data4D = data[0]\n",
        "    labels = bcvar    \n",
        "    bolddata_sl = data4D.reshape(sl_mask.shape[0] * sl_mask.shape[1] * sl_mask.shape[2], data[0].shape[3]).T    \n",
        "    t1 = time.time()\n",
        "    scores = []\n",
        "    # outer loop\n",
        "    sp = StratifiedKFold(n_splits=3)\n",
        "    for train, test in sp.split(bolddata_sl, labels):\n",
        "        train_data = bolddata_sl[train, :]\n",
        "        test_data = bolddata_sl[test, :]\n",
        "        train_label = labels[train]\n",
        "        test_label = labels[test]     \n",
        "        # inner loop\n",
        "        sp_train = StratifiedKFold(n_splits=2)\n",
        "        parameters = {'C':[0.01, 0.1, 1, 10]}\n",
        "        inner_clf = GridSearchCV(\n",
        "            SVC(kernel='linear'),\n",
        "            parameters,\n",
        "            cv=sp_train,\n",
        "            return_train_score=True)\n",
        "        inner_clf.fit(train_data, train_label)\n",
        "        # best C from inner loop\n",
        "        C_best = inner_clf.best_params_['C'] \n",
        "        # Train in outer loop\n",
        "        clf = SVC(kernel='linear', C=C_best)\n",
        "        clf.fit(train_data, train_label)\n",
        "        # Test in outer loop\n",
        "        scores.append(clf.score(test_data, test_label))\n",
        "    accuracy = np.mean(scores)\n",
        "    t2 = time.time()    \n",
        "#     print('Kernel duration: ' + str(t2 - t1) + \"\\n\\n\")\n",
        "    return accuracy\n",
        "\n",
        "print(\"Begin SearchLight\\n\")\n",
        "sl_result = sl.run_searchlight(calc_svm, pool_size=pool_size)\n",
        "print(\"End SearchLight\\n\")\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Print outputs\n",
        "print(\"Summarize searchlight results\")\n",
        "print(\"Number of searchlights run: \" + str(len(sl_result[mask==1])))\n",
        "print(\"Accuracy for each kernel function: \" + str(sl_result[mask==1].astype('double')))\n",
        "print('Total searchlight duration (including start up time): %.2f' % (end_time - begin_time))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fMCfgTNwcS-",
        "colab_type": "text"
      },
      "source": [
        "We have written a Python script for running searchlights. The file *searchlight.py* loads in number of participants, sets up the searchlight and kernel and then performs the searchlight across the multiple cores that the code is run on. \n",
        "\n",
        "This code performs leave-one-subject-out cross validation (i.e., the testing accuracy of subject 1 is based on the SVM classifier trained on subject 2 and 3; accuracy of subject 2 is based on the SVM trained on subject 1 and 3, etc.) The reason we are doing this cross-validation is that the single subject case as in the previous sections in this notebook is a case of double dipping. We only have one run of data for each subject, so the z-scoring within subject will cause double dipping in the single subject case.\n",
        "\n",
        "These results are saved for each subject separately. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O430wzmNwcS_",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 10:**  <a id=\"ex10\"></a> Run the `run_searchlight.sh` script using `sbatch run_searchlight.sh` in the command line. Once your analysis has finished (should take ~6 mins), load in your averaged searchlight results and plot it with the imported function [plotting](http://nilearn.github.io/modules/reference.html#module-nilearn.plotting) from nilearn. You will find function [plotting.plot_stat_map](https://nilearn.github.io/modules/generated/nilearn.plotting.plot_stat_map.html) useful. However, be aware that some of these functions assume the brain is MNI space. This face-scene dataset is not in MNI space, so if you overlay it on the default background the image will be 'outside of the brain'. To visualize this correctly, also provide the image as `bg_img` into this function. Also load in the averaged searchlight results across all 18 subjects we computed for you and plot it. The file is in the same directory as this notebook, called 'avg18_whole_brain_SL.nii.gz'. Use the same 'cut_coords', 'threshold', and 'vmax' across these two plots. You can start from threshold=0.65, cut_coords=(50,-40,10), vmax=1 for a good visualization. Compare the two plots. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv04SSOEwcS_",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 11:**  <a id=\"ex11\"></a> From your results, what do you infer about the discriminability of faces vs. scenes. Is it localized to a particular brain region?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM7elnhVwcS_",
        "colab_type": "text"
      },
      "source": [
        " <a id=\"ex12\"></a>\n",
        "**Exercise 12:** How do you figure out which searchlight cube is running on which rank when you run it on cluster? Write a searchlight_rank.py script to figure this out. Rewrite the kernel to return rank instead of accuracy and save the brain map as a nifty file. Run this on one subject only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlvyEQz7wcTA",
        "colab_type": "text"
      },
      "source": [
        "**Novel contribution:**<a name=\"novel\"></a> be creative and make one new discovery by adding an analysis, visualization, or optimization.\n",
        "\n",
        "Some ideas for novel contribution:  \n",
        "- Change the kernel to RSA and correlate it with a model RDM that distinguishes faces and scenes. Compare the RSA results with the decoding results.\n",
        "- Compute the confusion matrix for each searchlight. You can find useful information about confusion matrix [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html).\n",
        "- Run a searchlight, but not on every voxel. Run searchlights only on every third voxel, whole brain.\n",
        "- Run each subject on a different rank. BrainIAK searchlight has a way to do that.\n",
        "- Visualize the accuracy map using surface mapping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbhjV_VJwcTA",
        "colab_type": "text"
      },
      "source": [
        "## Contributions <a id=\"contributions\"></a> \n",
        "\n",
        "M. Kumar, C. Ellis and N. Turk-Browne produced the initial notebook 03/2018  \n",
        "T. Meissner minor edits  \n",
        "H. Zhang preprocessed dataset, add multi-subject section, add multiple exercises, add solutions, other edits  \n",
        "Vineet Bansal provided the MPI diagrams.  \n",
        "David Turner provided extensive input on how to use MPI for efficient running of jobs.  \n",
        "M. Kumar added MPI information and enhanced section contents.  \n",
        "K.A. Norman provided suggestions on the overall content and made edits to this notebook.  \n",
        "C. Ellis implemented updates from cmhn-s19.<br/>\n",
        "X. Li changed nipype function get_data() to get_fdata() since get_data() is deprecated in section 1.1.1"
      ]
    }
  ]
}